# -*- coding: utf-8 -*-
"""nlpmodel_BhumiDave.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J1HCK8Cky5DuWkOwVkThxYl01zleVdyD
"""

import pandas as pd
import nltk
nltk.download('punkt')
nltk.download('stopwords')
import sklearn
import numpy as np
from nltk.corpus import stopwords
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

#!pip install transformers
#!pip install torch
#!pip install nltk
'''
from google.colab import drive
drive.mount('/content/drive')
from google.colab import files
uploaded = files.upload()

from google.colab import drive
drive.mount('/content/drive')
from google.colab import drive
drive.mount('/gdrive')
'''

'''from google.colab import files
uploaded = files.upload()'''
url1="https://huggingface.co/datasets/bhumidad23/nlpmodel_dataset/blob/main/complaints.csv"
url2="https://huggingface.co/datasets/bhumidad23/nlpmodel_dataset/blob/main/final_dataframe.csv"
df=pd.read_csv(url1, encoding="utf-8", sep=",", on_bad_lines="skip")
df2=pd.read_csv(url2)

#Handling missing values
impute=SimpleImputer(strategy='most_frequent')
df=pd.DataFrame(impute.fit_transform(df),columns=df.columns)


#data preparation
x=df['narrative'].values
unique=df["product"].unique()
unique
df_encoded = pd.get_dummies(df['product'])
df = pd.concat([df, df_encoded], axis=1)

y = np.argmax(df[['credit_card', 'credit_reporting', 'debt_collection', 'mortgages_and_loans', 'retail_banking']].values, axis=1)

#data splitting
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=100)

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
x_train_vectorized = vectorizer.fit_transform(x_train)

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(
    n_estimators=50,         # fewer trees
    max_depth=20,            # limit depth
    max_features=5000,       # cap number of features considered per split
    n_jobs=-1,               # use all CPU cores
    random_state=42
)
#model.fit(x_train_vectorized, y_train)

#train the RfR
model=RandomForestClassifier()
model.fit(x_train_vectorized,y_train)

#Model evaluation
#making predictions
x_test_vectorized=vectorizer.transform(x_test) # to maintain consistency between training and test data
y_pred=model.predict(x_test_vectorized)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

#calculate metrics
accuracy=accuracy_score(y_test,y_pred)
precision=precision_score(y_test,y_pred,average='weighted')
recall=recall_score(y_test,y_pred,average='weighted')
f1=f1_score(y_test,y_pred,average='weighted')
report=classification_report(y_test,y_pred)

print(accuracy)
print(precision)
print(f1)
print(recall)
print(report)

#BERT
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import Trainer, TrainingArguments
import torch
from torch.utils.data import DataLoader,TensorDataset
labels=df['product']
label_encoder=LabelEncoder()
label_encoder.fit(labels)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

train_encodings = tokenizer(list(x_train), truncation=True, padding=True, max_length=128)
test_encodings = tokenizer(list(x_test), truncation=True, padding=True, max_length=128)

train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']),
                              torch.tensor(train_encodings['attention_mask']),
                              torch.tensor(y_train))
test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']),
                             torch.tensor(test_encodings['attention_mask']),
                             torch.tensor(y_test))

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16)

#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))

import torch
print(torch.cuda.is_available())  # True means CUDA (GPU) is available

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
for batch in train_loader:
    input_ids = batch[0].to(device)
    attention_mask = batch[1].to(device)
    labels_batch = batch[2].to(device)

    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels_batch)
    loss = outputs.loss
    logits = outputs.logits
    break


predictions = []

import torch
from torch.optim import AdamW
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

# --- Setup ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load tokenizer
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")

# Load model
model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=len(label_encoder.classes_)
).to(device)

# Freeze all encoder layers, only train classifier head
for param in model.distilbert.parameters():
    param.requires_grad = False

# Optimizer
optimizer = AdamW(model.parameters(), lr=2e-5)

# Training loop
epochs = 1   # just 1 epoch is often enough
for epoch in range(epochs):
    model.train()
    total_loss = 0

    for batch in train_loader:
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels_batch = batch[2].to(device)

        optimizer.zero_grad()
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels_batch
        )
        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()

    avg_train_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}, Loss: {avg_train_loss:.4f}")

# --- Evaluation ---
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

model.eval()
predictions = []
true_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels_batch = batch[2].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=-1)

        predictions.extend(preds.cpu().numpy())
        true_labels.extend(labels_batch.cpu().numpy())

# --- Metrics ---
print("\nDistilBERT Classifier Metrics:")
print("Accuracy:", accuracy_score(true_labels, predictions))
print("Precision:", precision_score(true_labels, predictions, average='weighted', zero_division=0))
print("Recall:", recall_score(true_labels, predictions, average='weighted', zero_division=0))
print("F1 Score:", f1_score(true_labels, predictions, average='weighted', zero_division=0))
print("\nClassification Report:\n", classification_report(true_labels, predictions, target_names=label_encoder.classes_, zero_division=0))

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Save metrics in a dictionary
best_model = {
    "accuracy": accuracy_score(y_test, y_pred),
    "precision": precision_score(y_test, y_pred, average='weighted'),
    "recall": recall_score(y_test, y_pred, average='weighted'),
    "f1_score": f1_score(y_test, y_pred, average='weighted')
    #"model_object": rf  # optional: store the trained model itself
}

# Check metrics
print(best_model)

import joblib
joblib.dump(best_model, 'nlp_bestmodel.pkl')
#!pip install streamlit
import streamlit as st

# Load the trained model
model = joblib.load("nlp_bestmodel.pkl")  # or "best_model.pkl"

# App title
st.title("Customer Complaint Classification")

# User input
user_text = st.text_area("Enter the complaint text:")

# Button to classify
if st.button("Classify"):
    if user_text.strip() == "":
        st.warning("Please enter some text!")
    else:
        # Assuming your model uses a vectorizer saved along with it
        # If you saved vectorizer separately:
        # vectorizer = joblib.load("vectorizer.pkl")
        # user_vector = vectorizer.transform([user_text])

        # Direct prediction if model handles raw text (like a pipeline)
        prediction = model.predict([user_text])
        prediction_proba = model.predict_proba([user_text])

        st.success(f"Predicted Complaint Category: {prediction[0]}")
        st.write("Prediction Probabilities:")
        for cls, prob in zip(model.classes_, prediction_proba[0]):
            st.write(f"{cls}: {prob:.2f}")
